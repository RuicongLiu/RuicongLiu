
# üìù Publications 
-----
<img style="float: left; margin:5px 10px" src="images/500x300.png" width="160" height="140">
### ActionVOS: Actions as Prompts for Video Object Segmentation
<p style="line-height:1.0">
<font size="2">
Liangyang Ouyang, <strong>Ruicong Liu</strong>, Yifei Huang, Ryosuke Furuta, Yoichi Sato <br />
European Conference on Computer Vision (<strong>ECCV</strong>), 2024<br />
<a href="https://arxiv.org/pdf/2407.07402">Paper</a> | 
<a href="https://github.com/ut-vision/ActionVOS">Code</a>
<br />
</font>
</p>

<img style="float: left; margin:5px 10px" src="images/500x300.png" width="160" height="140">
### Masked Video and Body-worn IMU Autoencoder for Egocentric Action Recognition
<p style="line-height:1.0">
<font size="2">
Mingfang Zhang, Yifei Huang, <strong>Ruicong Liu</strong>, Yoichi Sato <br />
European Conference on Computer Vision (<strong>ECCV</strong>), 2024 <br /> 
<a href="https://arxiv.org/pdf/2407.06628">Paper</a> 
<br />
</font>
</p>

<img style="float: left; margin:5px 10px" src="images/500x300.png" width="160" height="140">
### Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation
<p style="line-height:1.0">
<font size="2">
<strong>Ruicong Liu</strong>, Takehiko Ohkawa, Mingfang Zhang, Yoichi Sato <br />
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 <br /> 
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Single-to-Dual-View_Adaptation_for_Egocentric_3D_Hand_Pose_Estimation_CVPR_2024_paper.pdf">Paper</a> | 
<a href="https://www.youtube.com/watch?v=EzlmIre1PCY&t=25s">Video</a> |
<a href="https://github.com/ut-vision/S2DHand">Code</a>
<br />
</font>
</p>

<img style="float: left; margin:5px 10px" src="images/500x300.png" width="160" height="140">
### PnP-GA+: Plug-and-Play Domain Adaptation for Gaze Estimation using Model Variants
<p style="line-height:1.0">
<font size="2">
<strong>Ruicong Liu</strong>, Yunfei Liu, Haofei Wang, Feng Lu <br />
IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2024 <br /> 
<a href="https://ieeexplore.ieee.org/abstract/document/10378867/">Paper</a>
<!-- <a href="https://www.youtube.com/watch?v=EzlmIre1PCY&t=25s">Video</a> | -->
<!-- <a href="https://github.com/MickeyLLG/UVAGaze">Code</a> -->
<br />
</font>
</p>

<img style="float: left; margin:5px 10px" src="images/500x300.png" width="160" height="140">
### UVAGaze: Unsupervised 1-to-2 Views Adaptation for Gaze Estimation
<p style="line-height:1.0">
<font size="2">
<strong>Ruicong Liu</strong>, Feng Lu <br />
AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2024 <br /> 
<a href="https://arxiv.org/pdf/2312.15644">Paper</a> | 
<!-- <a href="https://www.youtube.com/watch?v=EzlmIre1PCY&t=25s">Video</a> | -->
<a href="https://github.com/MickeyLLG/UVAGaze">Code</a>
<br />
</font>
</p>

<img style="float: left; margin:5px 10px" src="images/papers/iccv-21.png" width="160" height="140">
### Generalizing gaze estimation with outlier-guided collaborative adaptation
<p style="line-height:1.0">
<font size="2">
Yunfei Liu*, <strong>Ruicong Liu*</strong>, Haofei Wang, Feng Lu <br />
IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2021 <br /> 
<a href="http://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Generalizing_Gaze_Estimation_With_Outlier-Guided_Collaborative_Adaptation_ICCV_2021_paper.pdf">Paper</a> | 
<!-- <a href="https://www.youtube.com/watch?v=EzlmIre1PCY&t=25s">Video</a> | -->
<a href="https://github.com/DreamtaleCore/PnP-GA">Code</a>
<br />
</font>
</p>

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation](https://arxiv.org/abs/2401.04747)

Junming Chen, **Yunfei Liu**, Jianan Wang, Ailing Zeng, Yu Li, Qifeng Chen

[**Project**](https://jeremycjm.github.io/proj/DiffSHEG)
- We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture generation with arbitrary length.
- Our diffusion-based co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/papers/iccv23-moda.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions](https://arxiv.org/abs/2307.10008)

**Yunfei Liu**, Lijian Lin, Fei Yu, Changyin Zhou, Yu Li

[**Project**](projects/iccv23-moda/index.html)
- We propose a unified system for multi-person, diverse, and high-fidelity talking portrait video generation. 
- Extensive evaluations demonstrate that the proposed system produces more natural and realistic video portraits compared to previous methods.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2023</div><img src='images/papers/tpami-23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[First- And Third-person Video Co-analysis By Learning Spatial-temporal Joint Attention](https://ieeexplore.ieee.org/document/9220850)

Huangyue Yu, Minjie Cai, **Yunfei Liu**, Feng Lu

**Project | IF=17.730** 
- We propose a multi-branch deep network, which extracts cross-view joint attention and shared representation from static frames with spatial constraints, in a self-supervised and simultaneous manner.
- We demonstrate how the learnt joint information can benefit various applications.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022</div><img src='images/papers/cvpr22-gazeonce.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[GazeOnce: Real-Time Multi-Person Gaze Estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_GazeOnce_Real-Time_Multi-Person_Gaze_Estimation_CVPR_2022_paper.pdf)

Mingfang Zhang, **Yunfei Liu**, Feng Lu

[**Project**](https://github.com/mf-zhang/GazeOnce) 
- GazeOnce is the first one-stage endto-end gaze estimation method.
- This unified framework not only offers a faster speed, but also provides a lower gaze estimation error compared with other SOTA methods.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2021</div><img src='images/papers/iccv-21.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation](http://arxiv.org/abs/2107.13780)

**Yunfei Liu**\*, Ruicong Liu\*, Haofei Wang, Feng Lu

[**Project**](projects/iccv21/index.html) <strong><span class='show_paper_citations' data='B1Z1vTMAAAAJ:Tyk-4Ss8FVUC'></span></strong>
- PnP-GA is an ensemble of networks that learn collaboratively with the guidance of outliers.
- Existing gaze estimation networks can be directly plugged into PnP-GA and generalize the algorithms to new domains.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2020</div><img src='images/papers/cvpr-20.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">


[Unsupervised Learning for Intrinsic Image Decomposition from a Single Image](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Unsupervised_Learning_for_Intrinsic_Image_Decomposition_From_a_Single_Image_CVPR_2020_paper.pdf)

**Yunfei Liu**, Yu Li, Shaodi You, Feng Lu

[**Project** ![](https://img.shields.io/github/stars/DreamtaleCore/USI3D?style=social)](projects/cvpr20/index.html) <strong><span class='show_paper_citations' data='B1Z1vTMAAAAJ:qjMakFHDy7sC'></span></strong>
- USI3D is the first intrinsic image decomposition method that learns from uncorrelected image sets.
- **Academic Impact:** This work is included by many low-level vision projects, such as [Relighting4D ![](https://img.shields.io/github/stars/FrozenBurning/Relighting4D?style=social)](https://github.com/FrozenBurning/Relighting4D), [IntrinsicHarmony](https://github.com/zhenglab/IntrinsicHarmony), [DIB-R++](https://nv-tlabs.github.io/DIBRPlus/). Discussions in [Zhihu](https://zhuanlan.zhihu.com/p/269632886).
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2020</div><img src='images/papers/eccv-20.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks](https://arxiv.org/abs/2007.02343)

**Yunfei Liu**, Xingju Ma, James Bailey, Feng Lu

[**Project | (Citations 300+**)] <strong><span class='show_paper_citations' data='ueU8eIwAAAAJ:d1gkVwhDpl0C'></span></strong>
- We¬†present¬†a¬†new¬†type¬†of¬†backdoor¬†attack: natural¬†reflection¬†phenomenon.
- **Academic Impact:** This work is included by many backdoor attack/defense works, Such as [NAD ![](https://img.shields.io/github/stars/bboylyg/NAD?style=social)](https://github.com/bboylyg/NAD). This work is at the first place at [google scholar](https://scholar.google.com.hk/scholar?hl=zh-CN&as_sdt=0%2C5&q=backdoor+attack&btnG=) .
</div>
</div>
- `CVPR2024` [A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing](https://stem-inv.github.io/page/), Maomao Li, Yu Li, Tianyu Yang, **Yunfei Liu**, Dongxu Yue, Zhihui Lin, Dong Xu
- `ICLR 2024` GPAvatar: Generalizable and Precise Head Avatar from Image(s), Xuangeng Chu, Yu Li, Ailing Zeng, Tianyu Yang, Lijian Lin, **Yunfei Liu**, Tatsuya Harada
- `ICCV 2023` [Accurate 3D Face Reconstruction with Facial Component Tokens](https://liuyunfei.net/), Tianke Zhang, Xuangeng Chu, **Yunfei Liu**, Lijian Lin, Zhendong Yang, _et al._.
- `arXiv 2023` [Visibility Enhancement for Low-light Hazy Scenarios](https://arxiv.org/abs/2308.00591), Chaoqun Zhuang, **Yunfei Liu**, Sijia Wen, Feng Lu.
- `CVMJ 2023` [Discriminative feature encoding for intrinsic image decomposition](https://phi-ai.buaa.edu.cn/publications/index.htm), Zhongji Wang, **Yunfei Liu**, Feng Lu.
- `CVPR 2022` [Generalizing Gaze Estimation with Rotation Consistency](https://openaccess.thecvf.com/content/CVPR2022/papers/Bao_Generalizing_Gaze_Estimation_With_Rotation_Consistency_CVPR_2022_paper.pdf), Yiwei Bao, **Yunfei Liu**, Haofei Wang, Feng Lu.
- `IEEE-VR 2022` [Reconstructing 3D Virtual Face with Eye Gaze from a Single Image](https://phi-ai.buaa.edu.cn/publications/index.htm), Jiadong Liang, **Yunfei Liu**, Feng Lu. [<font color=Red>Oral</font>]
- `TOMM 2022` [Semantic Guided Single Image Reflection Removal](https://dl.acm.org/doi/10.1145/3510821), **Yunfei Liu**, Yu Li, Shaodi You, Feng Lu, [GitHub](https://github.com/DreamtaleCore/SGRRN).
- `arXiv 2022` [Jitter Does Matter: Adapting Gaze Estimation to New Domains](https://arxiv.org/abs/2210.02082), Mingjie Xu, Haofei Wang, **Yunfei Liu**, Feng Lu.
- `ISAMR 2021` [3D Photography with One-shot Portrait Relighting](https://ieeexplore.ieee.org/document/9585876), **Yunfei Liu**, Sijia Wen, Feng Lu.
- `ISAMR 2021` [Edge-Guided Near-Eye Image Analysis for Head Mounted Displays](https://ieeexplore.ieee.org/document/9583797), Zhimin Wang\*, Yuxin Zhao\*, **Yunfei Liu**, Feng Lu. [<font color=Red>Oral</font>] [GitHub](https://github.com/zhaoyuhsin/Edge-Guided-Near-Eye-Image-Analysis-for-Head-Mounted-Displays), [Demo video](https://youtu.be/wV1kkvdW5WE)
- `BMVC 2021` [Separating Content and Style for Unsupervised Image-to-Image Translation](https://arxiv.org/abs/2110.14404), **Yunfei Liu**, Haofei Wang, Yang Yue, Feng Lu. [GitHub](https://github.com/DreamtaleCore/SCS-UIT).
- `arXiv 2021` [Unsupervised Two-Stage Anomaly Detection](https://arxiv.org/pdf/2103.11671.pdf), **Yunfei Liu**, Chaoqun Zhuang, Feng Lu, [GitHub](https://github.com/DreamtaleCore/UTAD).
- `arXiv 2021` [Cloud Sphere: A 3D Shape Representation via Progressive Deformation](https://arxiv.org/abs/2112.11133), Zongji Wang, **Yunfei Liu**, Feng Lu.
- `arXiv 2021` [Vulnerability of Appearance-based Gaze Estimation](https://arxiv.org/abs/2103.13134), Mingjie Xu, Haofei Wang, **Yunfei Liu**, Feng Lu.
- `AAAI 2020` [Separate In Latent Space: Unsupervised Single Image Layer Separation](https://arxiv.org/abs/1906.00734), **Yunfei Liu**, Feng Lu. [GitHub](https://github.com/DreamtaleCore/SILS) [<font color=Red>Oral</font>]
- `ICPR 2020` [Adaptive Feature Fusion Network for Gaze Tracking in Mobile Tablets](https://arxiv.org/abs/2103.11119), Yiwei Bao, Yihua Cheng, **Yunfei Liu**, Feng Lu. [GitHub](https://github.com/kirito12138/AFF-Net).
- `ACM-MM 2019` [What I See Is What You See: Joint Attention Learning for First and Third Person Video Co-analysis](https://arxiv.org/abs/1904.07424), Huangyue Yu, Minjie Cai, **Yunfei Liu**, Feng Lu. -->
