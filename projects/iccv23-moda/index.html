
<!doctype html>
<html>
<head>
<title>MODA</title>
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="description"
			content="MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions">
<link href="assets/bootstrap.min.css" rel="stylesheet" >
<script src="assets/jquery-3.2.1.min.js"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="assets/style.css" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="assets/global_site_tag.js"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-129271073-1');
</script>

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="MODA">
<meta name="twitter:description"
			content="MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions">
<meta name="twitter:image"
      content="figures/teaser.jpg">
<style>
.paperthumb {
  float:left; width: 120px; margin: 3px 10px 7px 0;
}
.paperdesc {
  clear: both;
}
:target {
     background-color: yellow;
}
.pdemo {
  overflow-x: auto; text-align: center;
}
.pdemo table {
  display:inline-table;
}
.pdemo td {
  padding: 5px;
}
.pdemo .btn.reveal {
  width: 100%;
}
.pdemo .btn {
  background: #999;
  color: white;
  margin-top: 5px;
  margin-bottom: 5px;
}
.pdemo .stack {
  position: relative;
}
.pdemo .btn.reveal[data-reveal^=i]:hover {
  background: rgb(6, 221, 221);
  color: black;
}
.pdemo .btn.reveal[data-reveal=a]:hover {
  background: lightgreen;
  color: black;
}
.pdemo .btn.reveal[data-reveal=f]:hover {
  background: lightblue;
  color: black;
}
.pdemo .overlay {
  position: absolute;
  left: 0;
  top: 0;
  opacity: 0;
  z-index: 1;
  transition: opacity 0.5s;
}
.pdemo .overlay.visible {
  opacity: 1;
}
</style>

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <p class="lead">
 <nobr><font color="#FF9933">M</font><font color="#66CCFF">O</font><font color="#FFCC00">D</font><font color="#33CC66">A</font>: <font color="#FF9933">M</font>apping-<font color="#66CCFF">O</font>nce Audio-driven Portrait Animation with <font color="#FFCC00">D</font>ual <font color="#33CC66">A</font>ttentions</nobr>
<address>
  <nobr><a href="http://liuyunfei.net"
  >Yunfei Liu</a><sup>1</sup>,</nobr>
  <nobr><a href="https://scholar.google.com.hk/citations?user=Xf5_TfcAAAAJ&hl=zh-CN"
  >Lijian Lin</a><sup>1</sup>,</nobr>
  <nobr>Fei Yu<sup>2</sup>,</nobr>
  <nobr>Changyin Zhou<sup>2</sup>,</nobr>
  <nobr><a href="https://yu-li.github.io/"
  >Yu Li</a><sup>1</sup></nobr>
 <br>
  <nobr><sup>1</sup><a href="https://www.idea.edu.cn">International Digital Economy Academy (IDEA), Shenzhen, China</a>,&nbsp;&nbsp;</nobr>
  <nobr><sup>2</sup><a href="https://www.vistring.ai">Vistring Inc., Hangzhou, China</a></nobr>
</address>
 </p>
 </div>
</div><!-- end nd-pageheader -->
<div class="container">

<div class="row">
<div class="col text-center">
<p>
  <a href="https://arxiv.org/abs/2307.10008" class="d-inline-block p-3 align-top"><img height="100" width="78" src="figures/paper-snap.png" style="border:1px solid" data-nothumb><br>ICCV 2023<br>Paper</a>
<!--   <a href="https://github.com/DreamtaleCore/MODA" class="d-inline-block p-3 align-top"><img height="100" width="78" src="figures/code-snap.png" style="border:1px solid" data-nothumb><br>Code and Data<br>Github</a> -->
</div>
</div>

<div class="row">
<div class="col">

<!-- <p style="text-align: center"><img src="figures/teaser.png" style="max-width:85%"></p> -->
<p style="text-align: center">
  <!-- <video style="max-width:85%" playsinline autoplay loop preload muted> -->
  <video style="max-width:85%" controls playsinline preload>
    <source src="./figures/teaser.mp4" type="video/mp4">
  </video>
</p>
<p style="text-align: center; margin-bottom: 50px;">
  We propose a mapping-once system with dual-attention for multimodal and high-fidelity portrait video animation.
</p>

<p> We propose a unified system for multi-person, diverse, and high-fidelity talking portrait generation. Extensive evaluations demonstrate that the proposed system produces more natural and
realistic video portraits compared to previous methods. </p>

<h2>Overview</h2>

<p>Given the subject figure and arbitrary audio, the proposed system generates the audio-driven portrait video.
</p>
<p style="text-align: center"><img src="figures/overview.png"
  style="max-width:85%"></p>
<p style="text-align: center; margin-bottom: 50px;">
  The proposed method is a three-stage system.
</p>


<p> Our method contains three stages, <em>i.e.</em>,
<ol>
  <li>Mapping-Once network with Dual Attentions (MODA) generates talking representation from given audio. In MODA, we design a dual-attention module to encode accurate mouth movements and diverse modalities. </li>
  <li>Facial composer network generates dense and detailed face landmarks.</li>
  <li>Temporal-guided renderer syntheses stable videos.</li>
</ol>  

<style>
.show-unit {
  overflow-x: auto;
}
.show-unit img {
  max-height: 80px;
}
.explain-unit {
  text-align: center;
  margin-bottom: 10px;
}
</style>


<h2>Network architecture</h2>

<p>Given the driven audio A and
subject condition S, MODA aims to map them into R (consists of lip movement, eye blinking, head pose, and torso)
with a single forward process. </p>

<p style="text-align: center"><img src="figures/moda-arch.png"
    style="max-width:85%"></p>
<p style="text-align: center; margin-bottom: 50px;">
Architecture of MODA network. Given an audio and subject condition, MODA generates four types of motions within a single forward process.
</p>

<p>The proposed DualAttn simultaneously learns one-to-one mapping (SpecAttn) for lip-sync
and one-to-many mapping (ProbAttn) for other movements.</p>


<h2>Numerical results</h2>

<p> Comparisons with state-of-the-art methods. â€  denotes our generated results with size 256x256 through a small
render. Best results are highlighted in <b>bold</b>. The number with <u>underline</u> denotes the second-best result.

<p style="text-align: center"><img src="figures/numerical-rlt.png"
    style="max-width:80%"></p>

<p><b>Visual results.</b> Visual comparison of 5 state-of-the-art methods.

<p style="text-align: center"><img src="figures/visual-cmp.png"
    style="max-width:60%"></p>

<!-- <a>
  <video style="width:97%;height:97%;" playsinline autoplay loop preload muted>
    <source src="./figures/visual-cmp.mp4" type="video/mp4">
  </video>
</a> -->

<h2>Video</h2>

<hr style="margin-top:0px">
<div class="text-center">
  <div style="position:relative;padding-top:56.25%;">
    <iframe src="https://www.youtube.com/embed/VO6m49VC3zw" allowfullscreen=""
      style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
  </div>
</div>

<h2>How to cite</h2>

<p>
<div class="card">
<h3 class="card-header">Bibtex</h3>
<div class="card-block">
<pre class="card-text clickselect">

@inproceedings{liu2023MODA,
  title={MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions},
  author={Liu, Yunfei and Lin, Lijian and Fei, Yu and Changyin, Zhou, and Yu, Li},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2023}
}
</pre>
</div>
</div>
</p>

</div>
</div>


</div> <!-- container -->

</body>
</html>
